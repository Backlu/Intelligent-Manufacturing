{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# base\n",
    "\n",
    "Copyright © 2019 Hsu Shih-Chieh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Issue**\n",
    "1. 在.py檔裡面, 要import utils裡面的功能時, 要寫成 from .utils import Bunch (前面要有一個.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, csv, sys, shutil, warnings, joblib, glob, ntpath, hashlib, chardet, cv2, random, datetime, json\n",
    "from os import environ, listdir, makedirs, getcwd\n",
    "from os.path import dirname, exists, expanduser, isdir, join, splitext\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from utils import Bunch\n",
    "from utils import checkencoding\n",
    "from utils import readTDMSasDF\n",
    "from utils import MCase\n",
    "import logging\n",
    "logging.basicConfig(level=\"ERROR\")\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image, ImageEnhance\n",
    "from skimage.filters import try_all_threshold, threshold_mean, threshold_local, threshold_minimum, threshold_otsu\n",
    "import segmentation_models as sm\n",
    "\n",
    "from collections import Counter, defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_facialbeauty(img_width=350, img_height=350, qty=None):\n",
    "    \"\"\"\n",
    "    專案：顏值預測\n",
    "    Load and return facialbeauty dataset (regerssion\n",
    "    .. versionadded:: 20191008\n",
    "\n",
    "    Parameters:\n",
    "        img_width - resize image width to img_width\n",
    "        img_height - resize image height to img_height\n",
    "\n",
    "    Returns:\n",
    "        data : Bunch, dictionary like data \n",
    "        \n",
    "    Example:\n",
    "        data = load_facialbeauty()\n",
    "    \"\"\"\n",
    "    module_path=dirname(__file__)\n",
    "    ratings = joblib.load(join(module_path, 'images/FacialBeauty/All_Ratings.pkl'))\n",
    "    labels_df = ratings.groupby('Filename')['Rating'].mean()    \n",
    "    n_samples = len(os.listdir(join(module_path, 'images/FacialBeauty/Images')))\n",
    "    if qty:\n",
    "        if n_samples > qty:\n",
    "            n_samples=qty\n",
    "    \n",
    "    data = np.empty((n_samples, img_width, img_height, 3), dtype=np.float32)\n",
    "    target = np.empty((n_samples, 1), dtype=np.float32)\n",
    "    data_names = []\n",
    "    for idx, imgpath in enumerate(glob.glob(os.path.join(module_path,'images/FacialBeauty/Images/*'))):\n",
    "        if idx >= n_samples:\n",
    "            break        \n",
    "        img = load_img(imgpath, target_size=(img_width, img_height))\n",
    "        img_array = img_to_array(img)\n",
    "        fname = ntpath.basename(imgpath)\n",
    "        rating = labels_df.loc[fname]\n",
    "        data[idx] = img\n",
    "        target[idx] = rating\n",
    "        data_names.append(fname)\n",
    "\n",
    "    with open(join(module_path,'descr', 'facialbeauty.rst')) as rst_file:\n",
    "        fdescr = rst_file.read()\n",
    "        \n",
    "    #n_testsamples = len(os.listdir(join(module_path, 'images/FacialBeauty/testimages')))        \n",
    "    n_testsamples = len(glob.glob(os.path.join(module_path,'images/FacialBeauty/testimages/*_new.png')))\n",
    "    test_data = np.empty((n_testsamples, img_width, img_height, 3), dtype=np.float32)\n",
    "    for idx, imgpath in enumerate(glob.glob(os.path.join(module_path,'images/FacialBeauty/testimages/*_new.png'))):\n",
    "        img = load_img(imgpath, target_size=(img_width, img_height))\n",
    "        img_array = img_to_array(img)\n",
    "        test_data[idx] = img\n",
    "    \n",
    "    data = Bunch(data=data, target=target, data_names=data_names, DESCR=fdescr, test_data=test_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_hotmelt():\n",
    "    \"\"\"\n",
    "    專案：十字彈片\n",
    "    Load and return hotmelt dataset (classification)\n",
    "    此處抓的資料是專案中期手動收集與標注的影像, 後期自動收錄的影像尚未放進來\n",
    "    .. versionadded:: 20191009\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    Returns:\n",
    "        data : Bunch, dictionary like data \n",
    "        \n",
    "    Example:\n",
    "        data = load_hotmelt()\n",
    "    \"\"\"    \n",
    "    \n",
    "    module_path=dirname(__file__)\n",
    "    #module_path=''\n",
    "    path = join(module_path, 'images/HotMelt/phase1')\n",
    "\n",
    "    datagen = ImageDataGenerator()\n",
    "    gen = datagen.flow_from_directory(path, target_size=(2050//3,2432//3), batch_size=30, shuffle=False, color_mode='rgb') \n",
    "    gen.batch_size = gen.samples\n",
    "    data,target=gen.next()\n",
    "    target_names = gen.class_indices\n",
    "    data_names = np.array(gen.filenames)\n",
    "\n",
    "    with open(join(module_path,'descr', 'hotmelt.rst')) as rst_file:\n",
    "        fdescr = rst_file.read()\n",
    "        \n",
    "    data = list(map(lambda x: x.astype('uint8'), data))    \n",
    "    \n",
    "    bunch = Bunch(data=data, target=target, data_names=data_names, DESCR=fdescr)\n",
    "    return bunch    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hotmelt_generator():\n",
    "    \"\"\"\n",
    "    專案：十字彈片\n",
    "    Load and return hotmelt dataset generator\n",
    "    此處抓的資料是專案中期手動收集與標注的影像, 後期自動收錄的影像尚未放進來\n",
    "    這個method練習實作data generator, 如果數據量非常大無法一次全部load到RAM裡面的時候, 可以改用generator的方式批次讀入. \n",
    "    \n",
    "    .. versionadded:: 20191011\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    Returns:\n",
    "        data : Bunch, dictionary like data \n",
    "        \n",
    "    Example:\n",
    "        data = load_hotmelt_generator()\n",
    "    \"\"\"       \n",
    "    #module_path=dirname(__file__)\n",
    "    module_path=''    \n",
    "    path = join(module_path, 'images/HotMelt/phase1')\n",
    "    classnames = sorted(os.listdir(path))    \n",
    "    def data_generator(batch_size, augfun, dtype='tr', oversampling=True):\n",
    "        '''data generator for fit_generator'''\n",
    "\n",
    "        imgpaths = []\n",
    "        targets = []\n",
    "        path = join(module_path, 'images/HotMelt/phase1')\n",
    "        classnames = sorted(os.listdir(path))\n",
    "        for i, f in enumerate(classnames):\n",
    "            path = join(module_path, f'images/HotMelt/phase1/{f}/*')\n",
    "            for p in glob.glob(path):\n",
    "                imgpaths.append(p)\n",
    "                targets.append(i)     \n",
    "        \n",
    "        imgpaths_tr, imgpaths_ts, targets_tr, targets_ts = train_test_split(imgpaths, targets, test_size=0.2, random_state=40)        \n",
    "        imgpaths_tr, imgpaths_val, targets_tr, targets_val = train_test_split(imgpaths_tr, targets_tr, test_size=0.2, random_state=40)\n",
    "        \n",
    "        if dtype=='tr':\n",
    "            if oversampling:\n",
    "                ros = RandomOverSampler()\n",
    "                imgpaths_tr, targets_tr = ros.fit_resample(np.reshape(imgpaths_tr, (-1, 1)), targets_tr)   \n",
    "                imgpaths_tr = imgpaths_tr.flatten()\n",
    "            imgpaths = imgpaths_tr\n",
    "            targets = targets_tr\n",
    "        elif dtype=='val':\n",
    "            imgpaths = imgpaths_val\n",
    "            targets = targets_val         \n",
    "        else:\n",
    "            imgpaths = imgpaths_ts\n",
    "            targets = targets_ts      \n",
    "        \n",
    "        targets = pd.get_dummies(targets).values                \n",
    "        n, i = len(imgpaths), 0\n",
    "        while True:\n",
    "            batch_img = []\n",
    "            batch_y = []\n",
    "            for b in range(batch_size):\n",
    "                if i==0:\n",
    "                    imgpaths, targets = shuffle(imgpaths, targets)\n",
    "                img  = load_img(imgpaths[i])\n",
    "                img_array = img_to_array(img)\n",
    "                y = targets[i]\n",
    "                batch_img.append(img_array)\n",
    "                batch_y.append(y)\n",
    "                i = (i+1) % n\n",
    "            batch_img=np.array(batch_img)\n",
    "            batch_y=np.array(batch_y)\n",
    "            X, Y = augfun(batch_img, batch_y, batch_size)\n",
    "            yield X,Y\n",
    "            \n",
    "    with open(join(module_path,'descr', 'hotmelt.rst')) as rst_file:\n",
    "        fdescr = rst_file.read()            \n",
    "            \n",
    "    bunch = Bunch(dataGenerator=data_generator, data_names=classnames, DESCR=fdescr)\n",
    "    return bunch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hotmeltyolodata():\n",
    "    \"\"\"\n",
    "    專案：十字彈片\n",
    "    Load and return hotmelt ROI dataset description file (train.txt)\n",
    "    這個function只讀取train.txt, 並回傳內容, 實際抓取數據在img_hotmelt_ROI.ipynb裡面的generator實作\n",
    "    .. versionadded:: 20191009\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    Returns:\n",
    "        data : Bunch, dictionary like data \n",
    "        \n",
    "    Example:\n",
    "        data = load_hotmeltyolodata()\n",
    "    \"\"\"        \n",
    "    module_path=dirname(__file__)\n",
    "    path = join(module_path, 'images/HotMelt/objectdetection/train.txt')    \n",
    "    with open(join(module_path,'descr', 'hotmelt_roi.rst')) as rst_file:\n",
    "        fdescr = rst_file.read()   \n",
    "        \n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()    \n",
    "        \n",
    "    bunch = Bunch(data=lines, DESCR=fdescr)    \n",
    "    return bunch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __yolo3_annotaion_generate__():\n",
    "    \"\"\"\n",
    "    專案：十字彈片\n",
    "    這個function只用來將labelImg標注好的檔案(xxx.xml)轉換成yolov3要用的格式(train.txt)\n",
    "    目前是offline使用\n",
    "    Example:\n",
    "        data = __yolo3_annotaion_generate__()\n",
    "    \"\"\"    \n",
    "    def convert_annotation(image_id, list_file):\n",
    "        in_file = open('images/HotMelt/objectdetection/label/%s.xml'%(image_id))\n",
    "        tree=ET.parse(in_file)\n",
    "        root = tree.getroot()\n",
    "        for obj in root.iter('object'):\n",
    "            difficult = obj.find('difficult').text\n",
    "            cls = obj.find('name').text\n",
    "            cls_id = classes.index(cls)\n",
    "            xmlbox = obj.find('bndbox')\n",
    "            b = (int(xmlbox.find('xmin').text), int(xmlbox.find('ymin').text), int(xmlbox.find('xmax').text), int(xmlbox.find('ymax').text))\n",
    "            list_file.write(\" \" + \",\".join([str(a) for a in b]) + ',' + str(cls_id))\n",
    "\n",
    "    classes = ['roi']\n",
    "    wd = getcwd()    \n",
    "    image_ids = os.listdir('images/HotMelt/objectdetection/img/')\n",
    "    list_file = open('images/HotMelt/objectdetection/train.txt', 'w')\n",
    "    cnt=0\n",
    "    for image_id in image_ids:\n",
    "        image_id = image_id[:-4]\n",
    "        xmlfile = 'images/HotMelt/objectdetection/label/%s.xml'%(image_id)\n",
    "        if not os.path.exists(xmlfile):\n",
    "            print('skip',image_id)\n",
    "            continue        \n",
    "        cnt=cnt+1\n",
    "        list_file.write('%s/images/HotMelt/objectdetection/img/%s.jpg'%(wd, image_id))    \n",
    "        convert_annotation(image_id, list_file)\n",
    "        list_file.write('\\n')\n",
    "    list_file.close()    \n",
    "#__yolo3_annotaion_generate__()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_germination():\n",
    "    \"\"\"\n",
    "    專案：植物工廠發芽率檢測\n",
    "    Load and return germination dataset\n",
    "    分為兩種影像, 一種是後期用8k攝影機拍攝的, 另一種是專案初期用iphone手機拍的\n",
    "    .. versionadded:: 20191010\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    Returns:\n",
    "        data : Bunch, dictionary like data\n",
    "        回傳兩種影像，8k與iphone\n",
    "        \n",
    "    Example:\n",
    "        data = load_germination()\n",
    "    \"\"\"    \n",
    "    \n",
    "    #module_path=dirname(__file__)\n",
    "    module_path=''\n",
    "    path = join(module_path, 'images/Germination/image/8k/*')\n",
    "    files = glob.glob(path)\n",
    "    data_8k=[]\n",
    "    data_8knames=[]\n",
    "    for p in files:\n",
    "        img  = load_img(p)\n",
    "        img_array = img_to_array(img)\n",
    "        fname = ntpath.basename(p)\n",
    "        data_8k.append(img_array)\n",
    "        data_8knames.append(fname)\n",
    "        \n",
    "    path = join(module_path, 'images/Germination/image/iphone/*')\n",
    "    files = glob.glob(path)\n",
    "    data_iphone=[]\n",
    "    data_iphonenames=[]\n",
    "    for p in files:\n",
    "        img  = load_img(p)\n",
    "        img_array = img_to_array(img)\n",
    "        fname = ntpath.basename(p)\n",
    "        data_iphone.append(img_array)\n",
    "        data_iphonenames.append(fname)        \n",
    "\n",
    "    with open(join(module_path,'descr', 'germination.rst')) as rst_file:\n",
    "        fdescr = rst_file.read()\n",
    "    \n",
    "    \n",
    "    bunch = Bunch(data_iphone=data_iphone, data_iphonenames=np.array(data_iphonenames), data_8k=data_8k, data_8knames=np.array(data_8knames), DESCR=fdescr)\n",
    "    return bunch    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rca():\n",
    "    \"\"\"\n",
    "    專案: 良率異常集中性分析\n",
    "    Load and return Level 10生產數據 dataset\n",
    "    \n",
    "    .. versionadded:: 20191011\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    Returns:\n",
    "        data : Bunch, dictionary like data\n",
    "        回傳三種數據: SFC, Parts, Test\n",
    "        \n",
    "    Example:\n",
    "        data = load_rca()\n",
    "    \"\"\"    \n",
    "    #module_path=dirname(__file__)\n",
    "    module_path=''\n",
    "    path = join(module_path, 'data/rca/data/IPPD-L10_PARTS_SFC.txt')    \n",
    "    parts_df = pd.read_csv(path, encoding=checkencoding(path))\n",
    "    path = join(module_path, 'data/rca/data/IPPD-L10_SFC.txt')    \n",
    "    sfc_df = pd.read_csv(path, encoding=checkencoding(path))\n",
    "    path = join(module_path, 'data/rca/data/IPPD-L10_TEST.txt')    \n",
    "    test_df = pd.read_csv(path, encoding=checkencoding(path))\n",
    "    \n",
    "    test_df.columns = ['SN','Station','Stationcode','Machine','start_time','end_time','isTestFail','symptom','desc','uploadtime','emp','ver1','ver2']\n",
    "    sfc_df.columns = ['ID','SN','WO','HH_Part','CUST_Part','assembline','scantime','na1','na2','product','floor']\n",
    "    parts_df.columns = ['ID','PARTSN','scantime','opid','assembly_station','part','HH_Part','CUST_Part','line','na1','na2'] \n",
    "        \n",
    "        \n",
    "    with open(join(module_path,'descr', 'rca.rst')) as rst_file:\n",
    "        fdescr = rst_file.read()\n",
    "        \n",
    "    bunch = Bunch(sfc=sfc_df, parts=parts_df, test=test_df, sfc_names=sfc_df.columns, parts_names=parts_df.columns, test_names=test_df.columns, DESCR=fdescr)        \n",
    "    return bunch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_newscommentary_v14(MAX_LENGTH = 40, BATCH_SIZE = 128):\n",
    "    '''\n",
    "    FIXME: drop_prec目前設定為80, for demo only, 之後再GPU上訓練要設定為0\n",
    "    \n",
    "    下載訓練數據, 格式為中英文配對的tuple list, 對訓練數據建立中英文字典\n",
    "    前處理數據\n",
    "        - 在example的前後加入BOS, EOS索引值\n",
    "        - 過濾掉長度超過40的example\n",
    "        - padding: padded_batch 函式能幫我們將每個batch裡頭的序列都補0到跟當下 batch 裡頭最長的序列一樣長。\n",
    "        - shuffle: 將examples洗牌確保隨機性\n",
    "    .. versionadded:: 20191012\n",
    "\n",
    "    Parameters:\n",
    "        MAX_LENGTH: 每個example保留的長度\n",
    "        BATCH_SIZE: training的批次大小\n",
    "\n",
    "    Returns:\n",
    "        data : Bunch, dictionary like data\n",
    "        - 訓練數據, 驗證數據, 英文字典, 中文字底, 說明文件\n",
    "        \n",
    "    Example:\n",
    "        data = load_newscommentary_v14()        \n",
    "    '''\n",
    "    module_path=''\n",
    "    output_dir = \"text/wmt2019/newscommentary_v14\"\n",
    "    download_dir = output_dir\n",
    "    en_vocab_file = os.path.join(output_dir, \"en_vocab\")\n",
    "    zh_vocab_file = os.path.join(output_dir, \"zh_vocab\")\n",
    "    config = tfds.translate.wmt.WmtConfig(\n",
    "        version=tfds.core.Version('0.0.3', experiments={tfds.core.Experiment.S3: False}),\n",
    "        language_pair=(\"zh\", \"en\"),\n",
    "        subsets={tfds.Split.TRAIN: [\"newscommentary_v14\"] }\n",
    "    )\n",
    "    builder = tfds.builder(\"wmt_translate\", config=config)\n",
    "    builder.download_and_prepare(download_dir=download_dir)\n",
    "    \n",
    "    train_perc, val_prec= 20, 1\n",
    "    drop_prec = 100 - train_perc - val_prec\n",
    "    split = tfds.Split.TRAIN.subsplit([train_perc, val_prec, drop_prec])\n",
    "    examples = builder.as_dataset(split=split, as_supervised=True)\n",
    "    train_examples, val_examples, _ = examples\n",
    "    try:\n",
    "        subword_encoder_en = tfds.features.text.SubwordTextEncoder.load_from_file(en_vocab_file)\n",
    "        print(f\"載入已建立的字典： {en_vocab_file}\")\n",
    "    except:\n",
    "        print(\"沒有已建立的字典，從頭建立。\")\n",
    "        subword_encoder_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "            (en.numpy() for en, _ in train_examples), target_vocab_size=2**13) # 有需要可以調整字典大小\n",
    "\n",
    "        subword_encoder_en.save_to_file(en_vocab_file)\n",
    "\n",
    "    print(f\"字典大小：{subword_encoder_en.vocab_size}\")\n",
    "    print(f\"前 10 個 subwords：{subword_encoder_en.subwords[:10]}\")\n",
    "\n",
    "    try:\n",
    "        subword_encoder_zh = tfds.features.text.SubwordTextEncoder.load_from_file(zh_vocab_file)\n",
    "        print(f\"載入已建立的字典： {zh_vocab_file}\")\n",
    "    except:\n",
    "        print(\"沒有已建立的字典，從頭建立。\")\n",
    "        subword_encoder_zh = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "            (zh.numpy() for _, zh in train_examples), target_vocab_size=2**13, \n",
    "            max_subword_length=1) # 每一個中文字就是字典裡的一個單位\n",
    "        subword_encoder_zh.save_to_file(zh_vocab_file)\n",
    "\n",
    "    print(f\"字典大小：{subword_encoder_zh.vocab_size}\")\n",
    "    print(f\"前 10 個 subwords：{subword_encoder_zh.subwords[:10]}\")\n",
    "    \n",
    "    def encode(en_t, zh_t):\n",
    "        '''\n",
    "        因為字典的索引從 0 開始，我們可以使用 subword_encoder_en.vocab_size 這個值作為 BOS 的索引值\n",
    "        用 subword_encoder_en.vocab_size + 1 作為 EOS 的索引值\n",
    "        '''\n",
    "        en_indices = [subword_encoder_en.vocab_size] + subword_encoder_en.encode(en_t.numpy()) + [subword_encoder_en.vocab_size + 1]\n",
    "        zh_indices = [subword_encoder_zh.vocab_size] + subword_encoder_zh.encode(zh_t.numpy()) + [subword_encoder_zh.vocab_size + 1]\n",
    "        return en_indices, zh_indices\n",
    "\n",
    "    def tf_encode(en_t, zh_t):\n",
    "        '''\n",
    "        在 `tf_encode` 函式裡頭的 `en_t` 與 `zh_t` 都不是 Eager Tensors, 要到 `tf.py_funtion` 裡頭才是\n",
    "        另外因為索引都是整數，所以使用 `tf.int64`\n",
    "        '''\n",
    "        return tf.py_function(encode, [en_t, zh_t], [tf.int64, tf.int64])\n",
    "\n",
    "    def filter_max_length(en, zh, max_length=MAX_LENGTH):\n",
    "        return tf.logical_and(tf.size(en) <= max_length, tf.size(zh) <= max_length)\n",
    "    \n",
    "    print('preprocess...')\n",
    "    BUFFER_SIZE = 15000\n",
    "    # 訓練集\n",
    "    train_dataset = (train_examples  # 輸出：(英文句子, 中文句子)\n",
    "                     .map(tf_encode) # 輸出：(英文索引序列, 中文索引序列)\n",
    "                     .filter(filter_max_length) # 同上，且序列長度都不超過 40\n",
    "                     .cache() # 加快讀取數據\n",
    "                     .shuffle(BUFFER_SIZE) # 將例子洗牌確保隨機性\n",
    "                     .padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1])) # 將 batch 裡的序列都 pad 到一樣長度\n",
    "                     .prefetch(tf.data.experimental.AUTOTUNE)) # 加速\n",
    "    # 驗證集\n",
    "    val_dataset = (val_examples\n",
    "                   .map(tf_encode)\n",
    "                   .filter(filter_max_length)\n",
    "                   .padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1])))    \n",
    "    \n",
    "    with open(join(module_path,'descr', 'wmt2019.rst')) as rst_file:\n",
    "        fdescr = rst_file.read()\n",
    "        \n",
    "    bunch = Bunch(train=train_dataset, val=val_dataset, subword_encoder_zh=subword_encoder_zh, subword_encoder_en=subword_encoder_en, DESCR=fdescr)\n",
    "    return bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_iaicnc():\n",
    "    def getsensordata(path):\n",
    "        dflist=[]\n",
    "        trainflist = os.listdir(path)\n",
    "        trainflist.sort()\n",
    "        print(path,len(trainflist))\n",
    "        for i, f in tqdm(enumerate(trainflist)):\n",
    "            if ('Train_B' in path):\n",
    "                if i>=225 and i<=256:\n",
    "                    continue\n",
    "                elif i>=11 and i<=20:\n",
    "                    continue\n",
    "            tmp = pd.read_csv(os.path.join(path,f), header=None)\n",
    "            tmp['id']=i\n",
    "            dflist.append(tmp)\n",
    "        data =pd.concat(dflist)\n",
    "        data = data.reset_index(drop=True)\n",
    "        data.columns = ['x','y','z','xg','yg','zg','v','id']\n",
    "        return data       \n",
    "    \n",
    "    module_path =''\n",
    "    cachename ='bunch_jupyter.pkl'\n",
    "    #module_path=dirname(__file__)\n",
    "    \n",
    "    \n",
    "    with open(join(module_path,'descr', 'iai_cnc.rst')) as rst_file:\n",
    "        fdescr = rst_file.read()\n",
    "        \n",
    "    if os.path.exists(join(module_path, f'data/iai_cnc/{cachename}')):\n",
    "        print('讀取暫存數據')\n",
    "        bunch = joblib.load(join(module_path, f'data/iai_cnc/{cachename}'))\n",
    "        pass\n",
    "    else:\n",
    "        print('重新讀取數據')\n",
    "        wearcols=['flute_1','flute_2','flute_3']\n",
    "        path = join(module_path, 'data/iai_cnc/Train_A/Train_A_wear.csv')\n",
    "        wear_A = pd.read_csv(path, usecols=wearcols)\n",
    "        wear_A['flute_mean']=wear_A[wearcols].mean(axis=1) \n",
    "        wear_A['flute_max']=wear_A[wearcols].max(axis=1)\n",
    "        wear_A['flute_min']=wear_A[wearcols].min(axis=1)\n",
    "\n",
    "        path = join(module_path, 'data/iai_cnc/Train_B/Train_B_wear.csv')\n",
    "        wear_B = pd.read_csv(path, usecols=wearcols)\n",
    "        wear_B = wear_B.drop(list(range(225,257)))\n",
    "        wear_B = wear_B.drop(list(range(11,21)))\n",
    "        wear_B['flute_mean']=wear_B[wearcols].mean(axis=1) \n",
    "        wear_B['flute_max']=wear_B[wearcols].max(axis=1)\n",
    "        wear_B['flute_min']=wear_B[wearcols].min(axis=1)\n",
    "\n",
    "        print('wear_A',wear_A.shape)\n",
    "        print('wear_B',wear_B.shape)\n",
    "\n",
    "        trA = getsensordata(join(module_path, 'data/iai_cnc/Train_A/Train_A'))\n",
    "        trB = getsensordata(join(module_path, 'data/iai_cnc/Train_B/Train_B'))\n",
    "        ts = getsensordata(join(module_path, 'data/iai_cnc/Test/Test'))\n",
    "\n",
    "        bunch = Bunch(trA=trA, trB=trB, ts=ts, wear_A=wear_A, wear_B=wear_B, data_names=trA.columns, wear_names=wear_B.columns, DESCR=fdescr)            \n",
    "        joblib.dump(bunch, join(module_path, f'data/iai_cnc/{cachename}'))   \n",
    "    return bunch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ADS_generator():\n",
    "    \"\"\"\n",
    "    專案：絕緣片瑕疵檢測\n",
    "    \n",
    "    .. versionadded:: 20191015\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    Returns:\n",
    "        data : Bunch, dictionary like data \n",
    "            - data_generator, test_data, descr \n",
    "        \n",
    "    Example:\n",
    "        data = load_ADS_generator()\n",
    "        tr_fen = data.dataGenerator\n",
    "        funs = Bunch(fillmeangray=fillmeangray, imgaugmentation=imgaugmentation, extractDefect=extractDefect)\n",
    "        batch = next(tr_fen(4, 'tr', funs=funs))\n",
    "    \"\"\"       \n",
    "    module_path=''\n",
    "    def data_generator(batch_size, dtype='tr', funs=None, preprocess=True):\n",
    "        '''data generator for fit_generator'''\n",
    "        extractDefect=funs.extractDefect\n",
    "        fillmeangray=funs.fillmeangray\n",
    "        imgaugmentation=funs.imgaugmentation       \n",
    "        IMG_HEIGHT, IMG_WIDTH = (224,224)\n",
    "        imgpaths = []\n",
    "        path = join(module_path, 'images/Insulatingpatch/training/*')\n",
    "        imgpaths = glob.glob(path)\n",
    "        imgpaths_tr, imgpaths_val = train_test_split(imgpaths, test_size=0.2, random_state=40)        \n",
    "        imgpaths = imgpaths_tr if dtype=='tr' else imgpaths_val\n",
    "        preprocess_input = sm.get_preprocessing('densenet169')\n",
    "        n, i = len(imgpaths), 0\n",
    "        while True:\n",
    "            batch_img = []\n",
    "            batch_mask = []\n",
    "            for b in range(batch_size):\n",
    "                if dtype =='tr':\n",
    "                    imgpaths = shuffle(imgpaths) if i==0 else imgpaths #全部看完一次之後打亂順序\n",
    "                img = cv2.imread(os.path.join(imgpaths[i], 'img.png'))\n",
    "                mask = cv2.imread(os.path.join(imgpaths[i], 'label.png'))\n",
    "                #剪出Defect圖像, Mask, 位置\n",
    "                mask_gray = cv2.cvtColor(mask,cv2.COLOR_RGB2GRAY)\n",
    "                ret,mask_binary = cv2.threshold(mask_gray, 5, 255, cv2.THRESH_BINARY )\n",
    "                mask = cv2.cvtColor(mask_binary,cv2.COLOR_GRAY2RGB)                                \n",
    "                defectImg, defectMask, (bottom, top, left, right) = extractDefect(img, mask)\n",
    "                defectHeight, defectWidth, _ = defectMask.shape\n",
    "                #平移defect位址,  #FIXME: 可以再加上defect影像大小縮放\n",
    "                augy = random.choice(range((img.shape[0]-defectHeight)))\n",
    "                augx = random.choice(range((img.shape[1]-defectWidth)))\n",
    "                img_aug=img\n",
    "                mask_aug=mask\n",
    "                if dtype =='tr':\n",
    "                    img_aug[bottom:top, left:right] = img_aug[augy:augy+defectHeight, augx:augx+defectWidth]\n",
    "                    img_aug[augy:augy+defectHeight, augx:augx+defectWidth] = defectImg\n",
    "                    mask_aug=np.zeros_like(mask)\n",
    "                    mask_aug[augy:augy+defectHeight, augx:augx+defectWidth] = defectMask\n",
    "                img_aug = fillmeangray(img_aug) #空白處填充平均灰色\n",
    "                img_aug = imgaugmentation(img_aug) if dtype =='tr' else Image.fromarray(img_aug) #影像輕微變化\n",
    "                #Resize\n",
    "                img_aug = img_aug.resize((IMG_HEIGHT, IMG_WIDTH), Image.NEAREST)\n",
    "                img_aug = np.array(img_aug)\n",
    "                img_aug = preprocess_input(img_aug) if preprocess else img_aug\n",
    "                mask_aug = Image.fromarray(mask_aug).resize((IMG_HEIGHT, IMG_WIDTH), Image.NEAREST)\n",
    "                mask_aug = np.array(mask_aug)\n",
    "                mask_zero = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "                mask_aug = np.maximum(mask_zero, mask_aug)            \n",
    "                batch_img.append(img_aug)\n",
    "                mask_aug = mask_aug[:,:,:1]\n",
    "                mask_aug = mask_aug//255\n",
    "                batch_mask.append(mask_aug)    \n",
    "                i = (i+1) % n\n",
    "            batch_img=np.array(batch_img)\n",
    "            batch_mask=np.array(batch_mask)\n",
    "            yield batch_img,batch_mask\n",
    "\n",
    "    path = join(module_path, 'images/Insulatingpatch/training/*')            \n",
    "    paths = glob.glob(path)\n",
    "    trimgQty = len(paths)\n",
    "    \n",
    "    path = join(module_path, 'images/Insulatingpatch/testing/*')\n",
    "    paths = glob.glob(path)\n",
    "    testImgs = []\n",
    "    for n, path in tqdm(enumerate(paths), total=len(paths)):\n",
    "        img = cv2.imread(path)\n",
    "        #img = fillmeangray(img)    \n",
    "        #img = Image.fromarray(img).resize((IMG_HEIGHT, IMG_WIDTH), Image.NEAREST)\n",
    "        testImgs.append(img)\n",
    "\n",
    "    with open(join(module_path,'descr', 'ads.rst')) as rst_file:\n",
    "        fdescr = rst_file.read()            \n",
    "            \n",
    "    bunch = Bunch(dataGenerator=data_generator, testImgs=testImgs, trimgQty=trimgQty, DESCR=fdescr)\n",
    "    return bunch\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cnc():\n",
    "    \"\"\"\n",
    "    專案：刀具壽命預測, 刀具全生命週期數據\n",
    "    \n",
    "    .. versionadded:: 20191017\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    Returns:\n",
    "        data : Bunch, dictionary like data \n",
    "            - plc data, vibration sensor data, plc column names, sensor column names descr \n",
    "        \n",
    "    Example:\n",
    "        data = load_cnc()\n",
    "    \"\"\"        \n",
    "    module_path=''\n",
    "    if os.path.exists(join(module_path, 'data/cnc/GL-A2-09/plcblocks.pkl')):\n",
    "        print('讀取暫存檔')\n",
    "        plcblocks = joblib.load(join(module_path, 'data/cnc/GL-A2-09/plcblocks.pkl'))\n",
    "        sensorblocks = joblib.load(join(module_path, 'data/cnc/GL-A2-09/sensorblocks.pkl'))\n",
    "    else: \n",
    "        print('重新讀取數據')\n",
    "        #1. Load Wear measurment\n",
    "        np.set_printoptions(suppress=True)\n",
    "        wearmeasure_path = join(module_path, 'data/cnc/wearmeasure_GL.pkl')\n",
    "        wearmearue = joblib.load(wearmeasure_path)\n",
    "\n",
    "        #2. Load PLC Data\n",
    "        plcdatapath = join(module_path, 'data/cnc/GL-A2-09/PLC/*')\n",
    "        dfs = []\n",
    "        for p in glob.glob(plcdatapath):\n",
    "            df = pd.read_csv(p, encoding='csbig5')\n",
    "            dfs.append(df)\n",
    "        plcdata = pd.concat(dfs, axis=0)\n",
    "\n",
    "        plcdata = plcdata.apply(pd.to_numeric, errors='ignore')\n",
    "        plcdata['毫秒']= plcdata['毫秒'].map(lambda x: str(int(x)))\n",
    "        plcdata['datetime'] = plcdata[['日期','時間','毫秒']].apply(' '.join, axis=1)\n",
    "        plcdata['datetime'] = plcdata['datetime'].map(lambda x: datetime.datetime.strptime(x, '%Y/%m/%d %H:%M:%S %f'))\n",
    "        plcdata = plcdata.sort_values(by='datetime')\n",
    "        plcdata.reset_index(drop=True, inplace=True)\n",
    "        plcdata['單節'] = plcdata['單節'].fillna('')\n",
    "        plcdic ={}\n",
    "        plcdic['x'], plcdic['y'], plcdic['z'], plcdic['feed'], plcdic['time'], plcdic['feedtrue'], plcdic['feedrate'], plcdic['gcode'], plcdic['sload'],plcdic['speed'],plcdic['cutv'],plcdic['speedtrue'] = zip(*plcdata[['X軸機械座標', 'Y軸機械座標', 'Z軸機械座標', '設定進給', 'datetime', '實際進給','進給率','單節','主軸負載','設定轉速','切削量','實際轉速']].values)\n",
    "        for k in plcdic.keys():\n",
    "            plcdic[k] = np.array(plcdic[k])\n",
    "\n",
    "        #3. Get Sensor Data  \n",
    "        sensordatapath = join(module_path, 'data/cnc/c/Sensor/*')\n",
    "        tdmslist = []\n",
    "        for subf in glob.glob(sensordatapath):\n",
    "            if not os.path.isdir(subf): \n",
    "                continue\n",
    "            for tdms in glob.glob(os.path.join(subf,'*')):\n",
    "                if not tdms.endswith('tdms'):\n",
    "                    continue\n",
    "                timestr = tdms[-17:-5]\n",
    "                timeobj = datetime.datetime.strptime(timestr, '%y%m%d%H%M%S')\n",
    "                tdmslist.append((tdms, timeobj))\n",
    "\n",
    "        tdmslist = sorted(tdmslist, key=lambda s: s[1])\n",
    "        tdmsFlist, tdmsTlist=zip(*tdmslist)\n",
    "        tdmsFlist, tdmsTlist=np.array(tdmsFlist), np.array(tdmsTlist)\n",
    "\n",
    "        #4. Get PLC/Sensor Blocks, 將每5秒數據切割成一個數據塊放入plcblocks\n",
    "        cols=['Spindle_S01']#['Spindle_S01','Spindle_S02','Spindle_S03']\n",
    "        startpoint=0\n",
    "        framesize=5 #每5秒一個數據塊\n",
    "        plcblocks=[]\n",
    "        sensorblocks = defaultdict(list)\n",
    "        print(plcdic['time'].shape[0])\n",
    "        with tqdm(total=plcdic['time'].shape[0]) as pbar:\n",
    "            while True:\n",
    "                #get PLC block\n",
    "                if startpoint >= plcdic['time'].shape[0]:\n",
    "                    break\n",
    "                starttime = plcdic['time'][startpoint]\n",
    "                endtime = starttime + datetime.timedelta(seconds=framesize) \n",
    "                blockidx = np.where(plcdic['time'][startpoint:] < endtime)[0] + startpoint\n",
    "                plcblock ={}\n",
    "                if len(blockidx)==0:\n",
    "                    break\n",
    "                for k in plcdic.keys():\n",
    "                    plcblock[k] = plcdic[k][blockidx]\n",
    "                startpoint = startpoint+len(blockidx)\n",
    "                pbar.update(len(blockidx))\n",
    "                #get corresponding sensor block\n",
    "                try:\n",
    "                    stridx, endidx = np.where( (tdmsTlist <= starttime))[0][-1], np.where( (tdmsTlist <= endtime))[0][-1]\n",
    "                    tdmsblocklist = tdmslist[stridx:endidx+1]\n",
    "                    datalist=[]\n",
    "                    for f, t in tdmsblocklist:\n",
    "                        data = readTDMSasDF(path = f, cols=cols)\n",
    "                        delta1, delta2=(starttime-t).total_seconds(), (endtime-t).total_seconds()\n",
    "                        data = data[(data.index>delta1) & (data.index<delta2)]\n",
    "                        datalist.append(data)\n",
    "                    tdms_df = pd.concat(datalist, ignore_index=False) if len(datalist)>0 else pd.DataFrame()      \n",
    "\n",
    "                except IndexError as error:\n",
    "                    tdms_df=pd.DataFrame()\n",
    "                except Exception as exception:\n",
    "                    tdms_df=pd.DataFrame()\n",
    "\n",
    "                if len(tdms_df)==0:\n",
    "                    continue\n",
    "\n",
    "                for c in cols:\n",
    "                    sensorblocks[c].append(tdms_df[c].values)\n",
    "                plcblocks.append(plcblock)\n",
    "\n",
    "        joblib.dump(plcblocks, join(module_path, 'data/cnc/GL-A2-09/plcblocks.pkl'))\n",
    "        joblib.dump(sensorblocks, join(module_path, 'data/cnc/GL-A2-09/sensorblocks.pkl'))\n",
    "\n",
    "    with open(join(module_path,'descr', 'cnc.rst')) as rst_file:\n",
    "        fdescr = rst_file.read()                    \n",
    "    bunch = Bunch(plc=plcblocks, sensor=sensorblocks, plcname = list(plcblocks[0].keys()), sensornames=list(sensorblocks.keys()), DESCR=fdescr)\n",
    "    return bunch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fcft():\n",
    "    \"\"\"\n",
    "    專案：主軸異常偵測, 主軸熱機數據 (20180430, 20180502, 20180503, 20180508)\n",
    "    \n",
    "    .. versionadded:: 20191018\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    Returns:\n",
    "        data : Bunch, dictionary like data \n",
    "            - vibration sensor data in each rpm speed, sensor column names, descr \n",
    "        \n",
    "    Example:\n",
    "        data = load_fcft()\n",
    "    \"\"\"        \n",
    "    module_path=''\n",
    "    if os.path.exists(join(module_path, 'data/fcft/bunch.pkl')):\n",
    "        print('讀取暫存檔')\n",
    "        bunch = joblib.load(join(module_path, 'data/fcft/bunch.pkl'))\n",
    "    else:     \n",
    "        #1. Load PLC Data\n",
    "        plcdatapath = join(module_path, 'data/fcft/*') \n",
    "        dfs = []\n",
    "        for subf in glob.glob(plcdatapath):\n",
    "            for p in glob.glob(join(subf, 'PLC/*')):\n",
    "                df = pd.read_csv(p, encoding='csbig5')\n",
    "                dfs.append(df) \n",
    "\n",
    "        plcdata = pd.concat(dfs, axis=0)\n",
    "        plcdata = plcdata.apply(pd.to_numeric, errors='ignore')\n",
    "        plcdata['毫秒']= plcdata['毫秒'].map(lambda x: str(int(x)))\n",
    "        plcdata['datetime'] = plcdata[['日期','時間','毫秒']].apply(' '.join, axis=1)\n",
    "        plcdata['datetime'] = plcdata['datetime'].map(lambda x: datetime.datetime.strptime(x, '%Y/%m/%d %H:%M:%S %f'))\n",
    "        plcdata = plcdata.sort_values(by='datetime')\n",
    "        plcdata.reset_index(drop=True, inplace=True)\n",
    "        plcdata['單節'] = plcdata['單節'].fillna('')\n",
    "\n",
    "        #2. Get Sensor Data  \n",
    "        print('  > 讀取sensor數據')\n",
    "        sensordatapath = plcdatapath = join(module_path, 'data/fcft/*') #join(module_path, 'data/fcft/GL_A2-FCFT0430/Sensor/*')\n",
    "        tdmslist = []\n",
    "        df_list = []\n",
    "        cols=['Spindle_S01','Spindle_S02','Spindle_S03']#['Spindle_S01','Spindle_S02','Spindle_S03','Current_IA','Current_IB','Current_IC']\n",
    "        for subf in glob.glob(sensordatapath):\n",
    "            for tdms in glob.glob(join(subf, 'Sensor/*')):\n",
    "                if not tdms.endswith('tdms'):\n",
    "                    continue\n",
    "                timestr = tdms[-17:-5]\n",
    "                timeobj = datetime.datetime.strptime(timestr, '%y%m%d%H%M%S')\n",
    "                tdmslist.append((tdms, timeobj))\n",
    "        tdmslist = sorted(tdmslist, key=lambda s: s[1])\n",
    "        for f,t in tqdm(tdmslist):\n",
    "            try:\n",
    "                df_ = readTDMSasDF(f, cols)\n",
    "                dtobj = t\n",
    "                df_['time'] = list(map(lambda x: x[0] + datetime.timedelta(0,x[1]), list(zip([dtobj]*len(df_.index), df_.index))))\n",
    "                df_list.append(df_)\n",
    "                print(df['time'].head())\n",
    "            except:\n",
    "                #print(f'warning! read {f} fail' )\n",
    "                pass\n",
    "        sensordata = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "        with open(join(module_path,'descr', 'fcft.rst')) as rst_file:\n",
    "            fdescr = rst_file.read()                    \n",
    "\n",
    "        bunch=Bunch( datanames=cols, DESCR=fdescr)\n",
    "        for speed in plcdata['設定轉速'].unique():\n",
    "            if speed==0:\n",
    "                continue\n",
    "            endtime = max(plcdata[plcdata['設定轉速']==speed]['datetime'])\n",
    "            starttime = min(plcdata[plcdata['實際轉速']>=speed]['datetime'])\n",
    "            tdata = sensordata[(sensordata['time']>=starttime) & (sensordata['time']<=endtime)]\n",
    "            bunch[f'speed_{speed}rpm']=tdata\n",
    "            \n",
    "        joblib.dump(bunch, join(module_path, 'data/fcft/bunch.pkl'))\n",
    "    return bunch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_moldcase(caseid = ['case01','case02']):\n",
    "    \"\"\"\n",
    "    專案：成型機PHM案例數據\n",
    "    \n",
    "    .. versionadded:: 20191018\n",
    "\n",
    "    Parameters:\n",
    "        - caseid: case id list, ['case01','case02', ...]\n",
    "        目前有case01 ~ case30數據, 其中case13, case14的健康值數據遺失\n",
    "\n",
    "    Returns:\n",
    "        data : Bunch, dictionary like data \n",
    "            - spccol_mapping: PLC數據欄位的中英文對照表\n",
    "            - caseinfo: 每一個case的詳細資訊\n",
    "            - caseXX_plc: 案例的PLC數據\n",
    "            - caseXX_HV: 案例的健康值數據\n",
    "    Example:\n",
    "        data = load_moldcase()\n",
    "    \"\"\"           \n",
    "    module_path=''\n",
    "    spccol_mapping = json.load(open('data/mold/spccol_mapping.json', 'r'))\n",
    "    caseinfo = json.load(open('data/mold/case.json', 'r'))\n",
    "    \n",
    "    with open(join(module_path,'descr', 'mold.rst')) as rst_file:\n",
    "        fdescr = rst_file.read()       \n",
    "    bunch=Bunch( spccol_mapping=spccol_mapping, caseinfo=caseinfo, DESCR=fdescr)\n",
    "        \n",
    "    cpath = join(module_path,'data/mold/case.json')\n",
    "    for cid in caseid:\n",
    "        df_plc = pd.read_csv(f'data/mold/casedata/{cid}_PLC.csv')\n",
    "        df_hv = pd.read_csv(f'data/mold/casedata/{cid}_HV.csv')\n",
    "        case = MCase(cid, cpath)\n",
    "        bunch[f'{cid}_plc']=df_plc\n",
    "        bunch[f'{cid}_hv']=df_hv\n",
    "        bunch[f'{cid}_caseinfo'] = case\n",
    "    return bunch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
